\section{Numerical Linear Algebra}

\subsection{Condition Numbers}
The \emph{condition number} of a matrix \(A\) measures how sensitive the solution of \(A \mathbf{x} = \mathbf{b}\) 
is to small changes in \(A\) or \(\mathbf{b}\). Formally, we define
\[
\kappa(A) = \|A\|\cdot\|A^{-1}\|.
\]
A large value of \(\kappa(A)\) indicates an ill-conditioned problem, meaning that even minor perturbations in \(A\) 
or \(\mathbf{b}\) can cause significant changes in the solution \(\mathbf{x}\). In the special case of the 2-norm, 
the condition number \(\kappa_2(A)\) can be expressed in terms of the largest and smallest singular values, 
\(\sigma_{\max}\) and \(\sigma_{\min}\):
\[
\kappa_2(A) = \frac{\sigma_{\max}}{\sigma_{\min}}.
\]

\subsection{Stability of Algorithms}
When we implement numerical algorithms in finite-precision arithmetic, we distinguish between different notions of stability:
\begin{itemize}
    \item \textbf{Backward Stability}: 
    The computed solution is the exact solution of a slightly perturbed version of the original problem. In other words, 
    the algorithm might introduce small changes to \(A\) or \(\mathbf{b}\), but once those changes are accounted for, 
    the final result is mathematically exact.
    \item \textbf{Forward Stability}: 
    The computed solution itself is close to the exact solution of the original (unperturbed) system.
    \item \textbf{Mixed Stability}: 
    Combines aspects of backward and forward stability, allowing some internal steps to be backward stable while also 
    maintaining forward accuracy where it matters most.
\end{itemize}

\subsection{Floating-Point Arithmetic}
All numerical computations must account for the limitations of finite-precision arithmetic. The IEEE 754 standard 
dictates how real numbers are stored and manipulated, including rules for rounding and handling of overflow/underflow. 
One key parameter is the \emph{machine epsilon} \(\epsilon\), which is the smallest number such that \(1 + \epsilon > 1\) 
in floating-point representation. Because of rounding, small errors can accumulate during matrix operations, sometimes 
leading to \emph{catastrophic cancellation} in subtraction of nearly equal values. This underscores the importance of 
algorithmic choices that minimize opportunities for numerical instability.

\subsection{Iterative Methods}
For many large-scale systems, iterative solvers are preferred over direct methods because they better exploit sparse 
matrices and can be more memory-efficient.

\paragraph{Stationary Methods.}
Classical methods such as Jacobi iteration, Gauss--Seidel, and Successive Over-Relaxation (SOR) update the solution 
vector in place based on the most recent values. For example, the Jacobi method updates each component \(x_i\) by 
isolating it from the rest of the system:
\[
x_i^{(k+1)} 
= \frac{1}{a_{ii}} \Bigl(b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}\Bigr).
\]
Gauss--Seidel refines each component sequentially using updated values immediately, while SOR introduces a 
relaxation parameter to potentially speed up convergence.

\paragraph{Krylov Subspace Methods.}
More advanced iterative methods build their search directions within a sequence of subspaces generated by repeated 
multiplication of the system matrix. Conjugate Gradient (CG) is a prototypical example for symmetric positive-definite 
systems, while GMRES and BiCGSTAB address more general, possibly nonsymmetric problems. These approaches can yield 
faster convergence, especially when combined with effective \emph{preconditioning}.

\subsection{Preconditioning}
Preconditioning aims to transform a linear system \(A \mathbf{x} = \mathbf{b}\) into a more tractable form for iterative 
solvers. One common strategy is to solve
\[
M^{-1} A \mathbf{x} = M^{-1} \mathbf{b},
\]
where \(M\approx A\) but is easier to invert or factorize. This transformation can significantly reduce the number of 
iterations needed. Popular preconditioners include incomplete LU (ILU) factorizations, diagonal (Jacobi) scaling, 
and multigrid methods.

\subsection{Parallel Computing Considerations}
Finally, large-scale linear algebra computations often run on parallel systems. Designing parallel algorithms requires:
\begin{itemize}
    \item Distributing data among processing elements,
    \item Minimizing communication overhead,
    \item Balancing the computational load,
    \item Optimizing cache usage.
\end{itemize}
These considerations help maintain high efficiency and numerical stability at scale.
